import type { ConversationLog } from "@/types/chat"
import { AudioManager } from "./audio-manager"
import { generateSpeechWithAzureOpenAI, generateMultipleSpeechWithAzureOpenAI } from "./openai"
import { getAudioPermission } from "./cookie-utils"

// ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆåã¨éŸ³å£°IDã®ãƒãƒƒãƒ”ãƒ³ã‚°
const VOICE_MAP: Record<string, string> = {
  "å¾Œè—¤": "onyx",
  "è¥¿æ‘": "nova", 
  "å±±ç”°": "alloy",
  "default": "verse"
}

// éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
interface AudioCache {
  blob: Blob
  url: string
  timestamp: number
}

// éŸ³å£°å†ç”Ÿã‚­ãƒ¥ãƒ¼ã®ã‚¢ã‚¤ãƒ†ãƒ 
interface QueueItem {
  text: string
  voiceId: string
  onStart?: () => void
  onEnd?: () => void
}

// é«˜åº¦ãªéŸ³å£°å†ç”Ÿç®¡ç†ã‚¯ãƒ©ã‚¹
class AdvancedVoicePlayer {
  private static instance: AdvancedVoicePlayer | null = null
  private audioContext: AudioContext | null = null
  private currentSource: AudioBufferSourceNode | null = null
  private gainNode: GainNode | null = null
  private audioCache = new Map<string, AudioCache>()
  private playbackQueue: QueueItem[] = []
  private isPlaying = false
  private preloadPromises = new Map<string, Promise<Blob>>()

  private constructor() {}

  static getInstance(): AdvancedVoicePlayer {
    if (!this.instance) {
      this.instance = new AdvancedVoicePlayer()
    }
    return this.instance
  }

  // AudioContextã®åˆæœŸåŒ–ï¼ˆAudioManagerã¨å…±æœ‰ï¼‰
  async initialize(): Promise<boolean> {
    if (this.audioContext && this.gainNode) return true

    try {
      const audioManager = AudioManager.getInstance()
      
      // AudioManagerã®åˆæœŸåŒ–çŠ¶æ…‹ã‚’ç¢ºèª
      if (!audioManager.isAudioUnlocked()) {
        console.warn("AudioManager is not unlocked. Initialize AudioManager first.")
        return false
      }

      // AudioManagerã®AudioContextã‚’ä½¿ç”¨
      this.audioContext = audioManager.getAudioContext()
      
      if (!this.audioContext) {
        console.error("Failed to get AudioContext from AudioManager")
        return false
      }

      // GainNodeã‚’ä½œæˆ
      this.gainNode = this.audioContext.createGain()
      this.gainNode.gain.value = getAudioPermission() ? 1.0 : 0.0
      this.gainNode.connect(this.audioContext.destination)

      console.log("AdvancedVoicePlayer initialized with shared AudioContext")
      return true
    } catch (error) {
      console.error("Failed to initialize AdvancedVoicePlayer:", error)
      return false
    }
  }

  // éŸ³å£°ã®ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œï¼‰
  preloadVoice(text: string, voiceId: string): void {
    const cacheKey = this.getCacheKey(text, voiceId)
    
    // ã™ã§ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¾ãŸã¯ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ä¸­ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if (this.audioCache.has(cacheKey) || this.preloadPromises.has(cacheKey)) {
      return
    }

    // ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰é–‹å§‹
    const preloadPromise = generateSpeechWithAzureOpenAI(text, voiceId)
      .then(blob => {
        const url = URL.createObjectURL(blob)
        this.audioCache.set(cacheKey, {
          blob,
          url,
          timestamp: Date.now()
        })
        this.preloadPromises.delete(cacheKey)
        return blob
      })
      .catch(error => {
        console.error("Preload failed:", error)
        this.preloadPromises.delete(cacheKey)
        throw error
      })

    this.preloadPromises.set(cacheKey, preloadPromise)
  }

  // ã‚­ãƒ¥ãƒ¼ã«éŸ³å£°ã‚’è¿½åŠ 
  async addToQueue(item: QueueItem): Promise<void> {
    console.log(`ğŸ“¤ [VOICE-QUEUE] Adding item to queue: "${item.text.substring(0, 30)}..." (Voice: ${item.voiceId})`)
    const addStart = performance.now()
    
    this.playbackQueue.push(item)
    console.log(`ğŸ“Š [VOICE-QUEUE] Queue length: ${this.playbackQueue.length}`)
    
    // æ¬¡ã®éŸ³å£°ã‚’ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ï¼ˆå…ˆèª­ã¿ï¼‰
    const preloadStart = performance.now()
    const nextIndex = this.playbackQueue.length
    if (nextIndex < this.playbackQueue.length + 3) { // æœ€å¤§3ã¤å…ˆã¾ã§å…ˆèª­ã¿
      const futureItems = this.playbackQueue.slice(nextIndex, nextIndex + 3)
      futureItems.forEach(futureItem => {
        console.log(`ğŸ”„ [VOICE-QUEUE] Preloading future item: "${futureItem.text.substring(0, 20)}..."`)
        this.preloadVoice(futureItem.text, futureItem.voiceId)
      })
    }
    const preloadTime = performance.now() - preloadStart
    console.log(`ğŸ”„ [VOICE-QUEUE] Preload setup: ${preloadTime.toFixed(2)}ms`)

    // å†ç”Ÿä¸­ã§ãªã‘ã‚Œã°é–‹å§‹
    if (!this.isPlaying) {
      console.log(`â–¶ï¸ [VOICE-QUEUE] Starting queue processing`)
      this.processQueue()
    } else {
      console.log(`â³ [VOICE-QUEUE] Queue is already playing, item will be processed in order`)
    }
    
    const addTime = performance.now() - addStart
    console.log(`âœ… [VOICE-QUEUE] Item added to queue: ${addTime.toFixed(2)}ms`)
  }

  // ã‚­ãƒ¥ãƒ¼ã®å‡¦ç†
  private async processQueue(): Promise<void> {
    if (this.isPlaying || this.playbackQueue.length === 0) {
      console.log(`âš ï¸ [VOICE-PROCESS] Queue processing skipped - Playing: ${this.isPlaying}, Queue length: ${this.playbackQueue.length}`)
      return
    }

    console.log(`ğŸµ [VOICE-PROCESS] Starting queue processing with ${this.playbackQueue.length} items`)
    const processStart = performance.now()

    this.isPlaying = true
    let itemsProcessed = 0

    while (this.playbackQueue.length > 0) {
      const item = this.playbackQueue.shift()!
      itemsProcessed++
      
      console.log(`ğŸµ [VOICE-PROCESS] Processing item ${itemsProcessed}: "${item.text.substring(0, 30)}..."`)
      const itemStart = performance.now()
      
      try {
        await this.playWithWebAudioAPI(item)
        const itemTime = performance.now() - itemStart
        console.log(`âœ… [VOICE-PROCESS] Item ${itemsProcessed} completed: ${itemTime.toFixed(2)}ms`)
      } catch (error) {
        const itemTime = performance.now() - itemStart
        console.error(`âŒ [VOICE-PROCESS] Playback error for item ${itemsProcessed} (${itemTime.toFixed(2)}ms):`, error)
        item.onEnd?.()
      }
    }

    this.isPlaying = false
    const totalProcessTime = performance.now() - processStart
    console.log(`âœ… [VOICE-PROCESS] Queue processing completed - ${itemsProcessed} items in ${totalProcessTime.toFixed(2)}ms`)
  }

  // Web Audio APIã‚’ä½¿ç”¨ã—ãŸå†ç”Ÿ
  private async playWithWebAudioAPI(item: QueueItem): Promise<void> {
    console.log(`ğŸ§ [WEB-AUDIO] Starting Web Audio API playback: "${item.text.substring(0, 30)}..."`)
    const webAudioStart = performance.now()
    
    if (!this.audioContext || !this.gainNode) {
      throw new Error("AudioContext not initialized")
    }

    const cacheKey = this.getCacheKey(item.text, item.voiceId)
    let audioCache = this.audioCache.get(cacheKey)

    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯ç”Ÿæˆ
    const cacheCheckStart = performance.now()
    if (!audioCache) {
      console.log(`ğŸ” [WEB-AUDIO] Cache miss for: ${cacheKey}`)
      
      // ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ä¸­ã®å ´åˆã¯å¾…æ©Ÿ
      const preloadPromise = this.preloadPromises.get(cacheKey)
      if (preloadPromise) {
        console.log(`â³ [WEB-AUDIO] Waiting for preload promise`)
        const preloadWaitStart = performance.now()
        await preloadPromise
        audioCache = this.audioCache.get(cacheKey)
        console.log(`âœ… [WEB-AUDIO] Preload wait completed: ${(performance.now() - preloadWaitStart).toFixed(2)}ms`)
      } else {
        // æ–°è¦ç”Ÿæˆ
        console.log(`ğŸµ [WEB-AUDIO] Generating new speech`)
        const ttsStart = performance.now()
        const blob = await generateSpeechWithAzureOpenAI(item.text, item.voiceId)
        const ttsTime = performance.now() - ttsStart
        console.log(`ğŸµ [WEB-AUDIO] Speech generation: ${ttsTime.toFixed(2)}ms`)
        
        const url = URL.createObjectURL(blob)
        audioCache = { blob, url, timestamp: Date.now() }
        this.audioCache.set(cacheKey, audioCache)
        console.log(`ğŸ’¾ [WEB-AUDIO] Audio cached with key: ${cacheKey}`)
      }
    } else {
      console.log(`âœ… [WEB-AUDIO] Cache hit for: ${cacheKey}`)
    }
    console.log(`ğŸ” [WEB-AUDIO] Cache operations: ${(performance.now() - cacheCheckStart).toFixed(2)}ms`)

    if (!audioCache) throw new Error("Failed to get audio data")

    // ArrayBufferã«å¤‰æ›
    const bufferStart = performance.now()
    const arrayBuffer = await audioCache.blob.arrayBuffer()
    const audioBuffer = await this.audioContext.decodeAudioData(arrayBuffer)
    const bufferTime = performance.now() - bufferStart
    console.log(`ğŸ”„ [WEB-AUDIO] Buffer conversion: ${bufferTime.toFixed(2)}ms`)

    // å‰ã®éŸ³å£°ã‚’åœæ­¢
    const stopStart = performance.now()
    if (this.currentSource) {
      try {
        this.currentSource.stop()
        this.currentSource.disconnect()
        console.log(`â¹ï¸ [WEB-AUDIO] Previous source stopped`)
      } catch (e) {
        // æ—¢ã«åœæ­¢ã—ã¦ã„ã‚‹å ´åˆã¯ç„¡è¦–
        console.log(`â„¹ï¸ [WEB-AUDIO] Previous source already stopped`)
      }
    }
    const stopTime = performance.now() - stopStart
    console.log(`â¹ï¸ [WEB-AUDIO] Source cleanup: ${stopTime.toFixed(2)}ms`)

    // æ–°ã—ã„ã‚½ãƒ¼ã‚¹ãƒãƒ¼ãƒ‰ã‚’ä½œæˆ
    const sourceCreateStart = performance.now()
    const source = this.audioContext.createBufferSource()
    source.buffer = audioBuffer
    source.connect(this.gainNode)
    const sourceCreateTime = performance.now() - sourceCreateStart
    console.log(`ğŸ”— [WEB-AUDIO] Source node creation: ${sourceCreateTime.toFixed(2)}ms`)

    const setupTime = performance.now() - webAudioStart
    console.log(`âš™ï¸ [WEB-AUDIO] Total setup time: ${setupTime.toFixed(2)}ms`)

    return new Promise<void>((resolve) => {
      const playbackStart = performance.now()
      
      source.onended = () => {
        const playbackTime = performance.now() - playbackStart
        const totalTime = performance.now() - webAudioStart
        console.log(`ğŸ [WEB-AUDIO] Playback ended - Duration: ${playbackTime.toFixed(2)}ms, Total: ${totalTime.toFixed(2)}ms`)
        
        this.currentSource = null
        item.onEnd?.()
        resolve()
      }

      console.log(`â–¶ï¸ [WEB-AUDIO] Starting audio playback`)
      item.onStart?.()
      source.start(0)
      this.currentSource = source
    })
  }

  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã®ç”Ÿæˆ
  private getCacheKey(text: string, voiceId: string): string {
    return `${voiceId}:${text.substring(0, 50)}`
  }

  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å­˜åœ¨ç¢ºèªï¼ˆå¤–éƒ¨ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ï¼‰
  hasCache(cacheKey: string): boolean {
    return this.audioCache.has(cacheKey)
  }

  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã®è¨­å®šï¼ˆå¤–éƒ¨ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ï¼‰
  setCacheData(cacheKey: string, data: AudioCache): void {
    this.audioCache.set(cacheKey, data)
    this.manageCacheSize()
  }

  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã®ç”Ÿæˆï¼ˆå¤–éƒ¨ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ï¼‰
  generateCacheKey(text: string, voiceId: string): string {
    return this.getCacheKey(text, voiceId)
  }

  // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  cleanup(): void {
    // ç¾åœ¨ã®å†ç”Ÿã‚’åœæ­¢
    if (this.currentSource) {
      try {
        this.currentSource.stop()
        this.currentSource.disconnect()
      } catch (e) {
        // ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
      }
    }

    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    this.audioCache.forEach(cache => {
      URL.revokeObjectURL(cache.url)
    })
    this.audioCache.clear()
    this.preloadPromises.clear()
    this.playbackQueue = []
    this.isPlaying = false

    // AudioContextã®ã‚¯ãƒ­ãƒ¼ã‚º
    if (this.audioContext && this.audioContext.state !== 'closed') {
      this.audioContext.close()
      this.audioContext = null
      this.gainNode = null
    }
  }

  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã®ç®¡ç†ï¼ˆå¤ã„ã‚‚ã®ã‹ã‚‰å‰Šé™¤ï¼‰
  private manageCacheSize(maxSize: number = 20): void {
    if (this.audioCache.size <= maxSize) return

    const entries = Array.from(this.audioCache.entries())
    entries.sort((a, b) => a[1].timestamp - b[1].timestamp)

    const toRemove = entries.slice(0, entries.length - maxSize)
    toRemove.forEach(([key, cache]) => {
      URL.revokeObjectURL(cache.url)
      this.audioCache.delete(key)
    })
  }
}

// ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
const voicePlayer = AdvancedVoicePlayer.getInstance()

// ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆåã‹ã‚‰éŸ³å£°IDã‚’å–å¾—
export const getVoiceIdByAssistant = (assistantName: string): string => {
  return VOICE_MAP[assistantName] || VOICE_MAP.default
}

// åˆæœŸåŒ–é–¢æ•°
export const initializeVoicePlayer = async (): Promise<boolean> => {
  const audioManager = AudioManager.getInstance()
  
  // AudioManagerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–ã‚’å¾…ã¤
  if (!audioManager.isAudioUnlocked()) {
    console.warn('AudioManager is not unlocked. VoicePlayer initialization will be skipped.')
    return false
  }

  return await voicePlayer.initialize()
}

// AudioManageråˆæœŸåŒ–å¾Œã«å‘¼ã³å‡ºã•ã‚Œã‚‹é–¢æ•°
export const initializeVoicePlayerAfterAudioPermission = async (): Promise<boolean> => {
  const audioManager = AudioManager.getInstance()
  
  // AudioManagerãŒç¢ºå®Ÿã«åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
  if (!audioManager.isAudioUnlocked()) {
    console.error('AudioManager is still not unlocked')
    return false
  }

  const success = await voicePlayer.initialize()
  if (success) {
    console.log('AdvancedVoicePlayer successfully initialized after audio permission')
  }
  
  return success
}

// è¤‡æ•°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ä¸€æ‹¬å†ç”Ÿï¼ˆä¸¦åˆ—TTSç”Ÿæˆã§å¤§å¹…é«˜é€ŸåŒ–ï¼‰
export const playAssistantMessages = async (
  messages: ConversationLog[],
  onSpeakerStart?: (assistantId: string) => void,
  onSpeakerEnd?: (assistantId: string) => void
): Promise<void> => {
  console.log(`ğŸµ [VOICE-PLAY] Starting playback for ${messages.length} messages`)
  const playbackStartTime = performance.now()

  // åˆæœŸåŒ–ç¢ºèª
  const initCheckStart = performance.now()
  const audioManager = AudioManager.getInstance()
  if (!audioManager.isAudioUnlocked()) {
    console.error('âŒ [VOICE-PLAY] AudioManager is not unlocked. Cannot play audio. Please grant audio permission first.')
    return
  }
  console.log(`âœ… [VOICE-PLAY] Audio manager check: ${(performance.now() - initCheckStart).toFixed(2)}ms`)

  // VoicePlayerã®åˆæœŸåŒ–ç¢ºèª
  const voiceInitStart = performance.now()
  const initialized = await initializeVoicePlayer()
  if (!initialized) {
    console.error('âŒ [VOICE-PLAY] Failed to initialize VoicePlayer')
    return
  }
  console.log(`âš™ï¸ [VOICE-PLAY] Voice player initialization: ${(performance.now() - voiceInitStart).toFixed(2)}ms`)

  // å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
  const filterStart = performance.now()
  const validMessages = messages.filter(
    msg => msg.role === 'assistant' && msg.speaker && msg.content
  )
  console.log(`ğŸ” [VOICE-PLAY] Message filtering: ${(performance.now() - filterStart).toFixed(2)}ms - ${validMessages.length}/${messages.length} valid`)

  if (validMessages.length === 0) {
    console.log('âš ï¸ [VOICE-PLAY] No valid messages to play')
    return
  }

  console.log(`ğŸš€ [VOICE-PLAY] Starting parallel TTS generation for ${validMessages.length} messages...`)

  // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ã¨ä¸¦åˆ—TTSç”Ÿæˆã®æº–å‚™
  const cacheCheckStart = performance.now()
  const messagesToGenerate: Array<{ message: ConversationLog; index: number }> = []
  const cachedMessages: Array<{ message: ConversationLog; index: number }> = []

  // å„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹ã‚’ç¢ºèª
  validMessages.forEach((message, index) => {
    const voiceId = getVoiceIdByAssistant(message.speaker!.name)
    const cacheKey = voicePlayer.generateCacheKey(message.content, voiceId)
    
    if (voicePlayer.hasCache(cacheKey)) {
      console.log(`âœ… [VOICE-PLAY] Cache hit for message ${index + 1}: ${message.speaker!.name}`)
      cachedMessages.push({ message, index })
    } else {
      console.log(`ğŸ”„ [VOICE-PLAY] Cache miss for message ${index + 1}: ${message.speaker!.name} - will generate`)
      messagesToGenerate.push({ message, index })
    }
  })
  console.log(`ğŸ” [VOICE-PLAY] Cache check completed: ${(performance.now() - cacheCheckStart).toFixed(2)}ms`)
  console.log(`ğŸ“Š [VOICE-PLAY] Cache status - Hit: ${cachedMessages.length}, Miss: ${messagesToGenerate.length}`)

  // ä¸¦åˆ—TTSç”Ÿæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ï¼‰
  if (messagesToGenerate.length > 0) {
    const speechRequests = messagesToGenerate.map(({ message }) => ({
      text: message.content,
      speaker: getVoiceIdByAssistant(message.speaker!.name),
      instructions: "æ—¥æœ¬äººã‚‰ã—ã„ç™ºå£°ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"
    }))

    try {
      // ä¸¦åˆ—ã§TTSç”Ÿæˆã‚’å®Ÿè¡Œ
      const ttsStart = performance.now()
      console.log(`ğŸ¯ [VOICE-PLAY] Generating ${messagesToGenerate.length} audio files in parallel...`)
      const speechResults = await generateMultipleSpeechWithAzureOpenAI(speechRequests)
      const ttsTime = performance.now() - ttsStart
      console.log(`âœ… [VOICE-PLAY] Parallel TTS generation completed: ${ttsTime.toFixed(2)}ms for ${messagesToGenerate.length} messages`)
      console.log(`âš¡ [VOICE-PLAY] Average TTS time per message: ${(ttsTime / messagesToGenerate.length).toFixed(2)}ms`)

      // ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
      const cacheStoreStart = performance.now()
      speechResults.forEach(result => {
        const { message } = messagesToGenerate[result.index]
        const voiceId = getVoiceIdByAssistant(message.speaker!.name)
        const cacheKey = voicePlayer.generateCacheKey(message.content, voiceId)
        
        // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        const url = URL.createObjectURL(result.blob)
        voicePlayer.setCacheData(cacheKey, {
          blob: result.blob,
          url,
          timestamp: Date.now()
        })
        console.log(`ğŸ’¾ [VOICE-PLAY] Cached audio for message ${messagesToGenerate[result.index].index + 1}`)
      })
      console.log(`ğŸ’¾ [VOICE-PLAY] Cache storage completed: ${(performance.now() - cacheStoreStart).toFixed(2)}ms`)

    } catch (error) {
      console.error('âŒ [VOICE-PLAY] Parallel TTS generation failed:', error)
      console.log('âš ï¸ [VOICE-PLAY] Falling back to sequential TTS generation...')
      // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã¯ã‚­ãƒ¥ãƒ¼ã§è‡ªå‹•çš„ã«è¡Œã‚ã‚Œã‚‹
    }
  }

  // å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…ƒã®é †åºã§ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã—ã¦å†ç”Ÿ
  const queueStart = performance.now()
  console.log(`ğŸ“¤ [VOICE-PLAY] Adding all messages to playback queue...`)
  
  for (let i = 0; i < validMessages.length; i++) {
    const message = validMessages[i]
    const voiceId = getVoiceIdByAssistant(message.speaker!.name)
    const assistantId = message.speaker!.id

    const addStart = performance.now()
    await voicePlayer.addToQueue({
      text: message.content,
      voiceId,
      onStart: () => {
        console.log(`â–¶ï¸ [VOICE-PLAY] Started playing: ${message.speaker!.name} (${i + 1}/${validMessages.length})`)
        onSpeakerStart?.(assistantId)
      },
      onEnd: () => {
        console.log(`â¹ï¸ [VOICE-PLAY] Finished playing: ${message.speaker!.name} (${i + 1}/${validMessages.length})`)
        onSpeakerEnd?.(assistantId)
      }
    })
    
    const addTime = performance.now() - addStart
    console.log(`ğŸ“¤ [VOICE-PLAY] Queue add ${i + 1}: ${addTime.toFixed(2)}ms`)
  }
  
  const totalQueueTime = performance.now() - queueStart
  const totalPlaybackTime = performance.now() - playbackStartTime
  console.log(`âœ… [VOICE-PLAY] All messages queued: ${totalQueueTime.toFixed(2)}ms`)
  console.log(`ğŸ“Š [VOICE-PLAY] Total playback setup time: ${totalPlaybackTime.toFixed(2)}ms`)
  console.log(`ğŸµ [VOICE-PLAY] Playback started for ${validMessages.length} messages`)
  
  // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼
  console.log(`\nğŸ“ˆ [VOICE-PLAY] === PERFORMANCE SUMMARY ===`)
  console.log(`ğŸ“Š [VOICE-PLAY] Total setup time: ${totalPlaybackTime.toFixed(2)}ms`)
  console.log(`ğŸ“Š [VOICE-PLAY] Cache hits: ${cachedMessages.length}/${validMessages.length}`)
  console.log(`ğŸ“Š [VOICE-PLAY] TTS generations: ${messagesToGenerate.length}`)
  if (messagesToGenerate.length > 0) {
    console.log(`ğŸ“Š [VOICE-PLAY] Parallel TTS speedup: ~${messagesToGenerate.length}x faster than sequential`)
  }
  console.log(`ğŸ“ˆ [VOICE-PLAY] ========================\n`)
}

// ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãã®å®‰å…¨ãªå†ç”Ÿé–¢æ•°
export const safePlayAssistantMessages = (
  messages: ConversationLog[],
  onSpeakerStart?: (assistantId: string) => void,
  onSpeakerEnd?: (assistantId: string) => void
): void => {
  console.log(`ğŸ›¡ï¸ [SAFE-VOICE] Starting safe voice playback wrapper for ${messages.length} messages`)
  const safePlayStart = performance.now()
  
  if (messages.length === 0) {
    console.log('âš ï¸ [SAFE-VOICE] No messages provided, skipping playback')
    return
  }

  playAssistantMessages(messages, onSpeakerStart, onSpeakerEnd)
    .then(() => {
      const safePlayTime = performance.now() - safePlayStart
      console.log(`âœ… [SAFE-VOICE] Safe voice playback wrapper completed: ${safePlayTime.toFixed(2)}ms`)
    })
    .catch(error => {
      const safePlayTime = performance.now() - safePlayStart
      console.error(`âŒ [SAFE-VOICE] Voice playback failed (${safePlayTime.toFixed(2)}ms):`, error)
    })
}

// ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆã‚¢ãƒ—ãƒªçµ‚äº†æ™‚ã«å‘¼ã³å‡ºã™ï¼‰
export const cleanupVoicePlayer = (): void => {
  voicePlayer.cleanup()
}
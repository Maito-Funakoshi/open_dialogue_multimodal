// @ts-ignore - Azure OpenAI types may not be available
import AzureOpenAI from "openai";
import type { ConversationLog } from "@/types/chat";
import { 
  ASSISTANTS, 
  USER, 
  GENDER, 
  SITUATION, 
  WORDINGS_PROMPT, 
  NAME_INDEX, 
  REFLECTING_CONVERSATION_COUNT 
} from "@/lib/config";

// ç’°å¢ƒå¤‰æ•°ã®å–å¾—
const AZURE_OPENAI_API_KEY = process.env.NEXT_PUBLIC_AZURE_OPENAI_API_KEY!;
const AZURE_OPENAI_API_VERSION = process.env.NEXT_PUBLIC_AZURE_OPENAI_API_VERSION!;
const AZURE_OPENAI_ENDPOINT = process.env.NEXT_PUBLIC_AZURE_OPENAI_ENDPOINT!;
const AZURE_DEPLOYMENT_NAME = process.env.NEXT_PUBLIC_AZURE_DEPLOYMENT_NAME!;

// TTSé–¢é€£ã®ç’°å¢ƒå¤‰æ•°
const AZURE_OPENAI_TTS_ENDPOINT = process.env.NEXT_PUBLIC_AZURE_OPENAI_TTS_ENDPOINT;
const AZURE_OPENAI_TTS_API_KEY = process.env.NEXT_PUBLIC_AZURE_OPENAI_TTS_API_KEY;
const AZURE_OPENAI_TTS_API_VERSION = process.env.NEXT_PUBLIC_AZURE_OPENAI_TTS_API_VERSION;
const AZURE_OPENAI_TTS_DEPLOYMENT_NAME = process.env.NEXT_PUBLIC_AZURE_OPENAI_TTS_DEPLOYMENT_NAME || "";

const CHARACTERS = {
    [ASSISTANTS[0].name]: ASSISTANTS[0].character,
    [ASSISTANTS[1].name]: ASSISTANTS[1].character,
    [ASSISTANTS[2].name]: ASSISTANTS[2].character,
};

// å‹•çš„ãªSITUATIONã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
function createSituation(userName: string, userGender: string): string {
    return `ã‚ªãƒ¼ãƒ—ãƒ³ãƒ€ã‚¤ã‚¢ãƒ­ãƒ¼ã‚°ãŒè¡Œã‚ã‚Œã‚‹å ´æ‰€
${userName}ã•ã‚“ã¯${userGender}ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§${ASSISTANTS[0].name}ã€${ASSISTANTS[1].name}ã€${ASSISTANTS[2].name}ã¯ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ`;
}

// å‹•çš„ãªCHAT_PROMPTã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
function createChatPrompt(userName: string, userGender: string): string {
    const situation = createSituation(userName, userGender);
    return `
    ## å¯¾è©±å ´é¢ã‚„çŠ¶æ³è¨­å®š
    ${situation}

    ## å½¢å¼ã®æ¡ä»¶
    - 1å€‹ä»¥ä¸Š10å€‹ä»¥ä¸‹ã®ç™ºè¨€ã‚’å«ã‚€ä¼šè©±ã®ã‚„ã‚Šå–ã‚Šã‚’ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ä¸‹ã•ã„ã€‚
        \`\`\`
        è©±è€…ã®åå‰ï¼šç™ºè¨€å†…å®¹
        è©±è€…ã®åå‰ï¼šç™ºè¨€å†…å®¹
        è©±è€…ã®åå‰ï¼šç™ºè¨€å†…å®¹
        \`\`\`
    - å¿…ãšç™ºè¨€ã—ãŸè©±è€…ã®åå‰ã‚’å«ã‚ã¦ä¸‹ã•ã„ã€‚
    - å¿…ãšå„ç™ºè¨€ã«ãŠã„ã¦${ASSISTANTS[0].name}ã€${ASSISTANTS[1].name}ã€${ASSISTANTS[2].name}ã®ã„ãšã‚Œã‹ã‚’è©±è€…ã®åå‰ã¨ã—ã¦ä¸‹ã•ã„ã€‚
    - è©±è€…ã®é †ç•ªã¯ãƒ©ãƒ³ãƒ€ãƒ ã§è‰¯ã„ã§ã™ã€‚ä¾‹ãˆã°${ASSISTANTS[2].name}ã§ã‚‚è‰¯ã„ã—${ASSISTANTS[1].name}â†’${ASSISTANTS[0].name}ã§ã‚‚è‰¯ã„ã§ã™ã€‚
    - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ç™ºè¨€ã«åŸºã¥ãã€æ–‡è„ˆä¸Šæœ€ã‚‚è¿”ä¿¡ã‚’ã™ã‚‹ã®ãŒç›¸å¿œã—ã„è€…ãŒè¿”ä¿¡ã™ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

    ## å†…å®¹ã®æ¡ä»¶
    - 1ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå¯¾3ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã„ã†æ§‹å›³ã§ã¯ãªã4äººãŒçš†å¯¾ç­‰ãªç«‹å ´ã«ç«‹ã£ãŸå¯¾è©±ã‚’å¿ƒãŒã‘ã¦ä¸‹ã•ã„ã€‚
    - æœ€å¾Œã®è©±è€…ä»¥å¤–ã¯çµ¶å¯¾ã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«è³ªå•ã—ãªã„ã§ä¸‹ã•ã„ã€‚ãªãœãªã‚‰ã‚ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŒã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«è³ªå•ã‚’æŠ•ã’ã‹ã‘ãŸç›´å¾Œã«ä»–ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŒã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å¿œç­”ã‚’é®ã£ã¦è©±ã—å§‹ã‚ã‚‹ã®ã¯ä¸è‡ªç„¶ã ã‹ã‚‰ã§ã™ã€‚
    - å‡ºåŠ›ã«å¯¾ã—ã¦è¿”ç­”ã™ã‚‹ã®ã¯${userName}ã ã‘ãªã®ã§ã€ã¾ãŸ${userName}ä»¥å¤–ã«è³ªå•ã‚’æŠ•ã’ã‹ã‘ã¦å‡ºåŠ›ã‚’çµ‚äº†ã™ã‚‹ã®ã¯çµ¶å¯¾ã‚„ã‚ã¦ä¸‹ã•ã„ã€‚
    - è¤‡æ•°äººãŒç™ºè©±ã™ã‚‹å ´åˆã¯ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆåŒå£«ã®å¯¾è©±ã‚’å¿ƒãŒã‘ã¦ä¸‹ã•ã„ã€‚
    - å¥³æ€§ãŒç™ºè¨€ã™ã‚‹å ´åˆã€ä¸€äººç§°ã¯å¿…ãšã€Œç§ã€ã«ã—ã¦ãã ã•ã„ã€‚
    - å¿ƒä¸­èªã¯å‡ºåŠ›ã›ãšã«ç™ºè¨€å†…å®¹ã ã‘ã‚’å‡ºåŠ›ã™ã‚‹ã‚ˆã†ã«ã—ã¦ä¸‹ã•ã„ã€‚
    - ${WORDINGS_PROMPT}
`;
}

// å‹•çš„ãªREFLECTING_PROMPTã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
function createReflectingPrompt(userName: string, userGender: string): string {
    const situation = createSituation(userName, userGender);
    return `
    ## å¯¾è©±å ´é¢ã‚„çŠ¶æ³è¨­å®š
    ${situation}
    - ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®${ASSISTANTS.length}äººã¯ã¾ã‚‹ã§ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒãã®å ´ã«ã¯ã„ãªã„ã‹ã®å¦‚ãä¼šè©±ã‚’è¡Œã„ã¾ã™ã€‚
    - ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŸã¡ã¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è©±ã‚’å…ƒã«ã€ä»–ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«è‡ªèº«ã®æ„è¦‹ã‚’ç™ºä¿¡ã—ã¾ã™ã€‚
    - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŸã¡ã®ä¼šè©±ã‚’å´ã‹ã‚‰èãã ã‘ã§ã€å¿œç­”ã—ãŸã‚Šã¯ã—ã¾ã›ã‚“ã€‚

    ## å½¢å¼ã®æ¡ä»¶
    - 10å€‹ä»¥ä¸Šã®ç™ºè¨€ã‚’å«ã‚€å¤§é‡ã®ä¼šè©±ã®ã‚„ã‚Šå–ã‚Šã‚’ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ä¸‹ã•ã„ã€‚
        \`\`\`
        è©±è€…ã®åå‰ï¼šç™ºè¨€å†…å®¹
        è©±è€…ã®åå‰ï¼šç™ºè¨€å†…å®¹
        è©±è€…ã®åå‰ï¼šç™ºè¨€å†…å®¹
        ãƒ»ãƒ»ãƒ»
        \`\`\`
    - å¿…ãšç™ºè¨€ã—ãŸè©±è€…ã®åå‰ã‚’å«ã‚ã¦ä¸‹ã•ã„ã€‚
    - å¿…ãšå„ç™ºè¨€ã«ãŠã„ã¦${ASSISTANTS[0].name}ã€${ASSISTANTS[1].name}ã€${ASSISTANTS[2].name}ã®ã„ãšã‚Œã‹ã‚’è©±è€…ã®åå‰ã¨ã—ã¦ä¸‹ã•ã„ã€‚
    - è©±è€…ã®é †ç•ªã¯ãƒ©ãƒ³ãƒ€ãƒ ã§è‰¯ã„ã§ã™ã€‚ä¾‹ãˆã°${ASSISTANTS[2].name}ã§ã‚‚è‰¯ã„ã—${ASSISTANTS[0].name}â†’${ASSISTANTS[1].name}â†’${ASSISTANTS[0].name}â†’${ASSISTANTS[2].name}ã§ã‚‚è‰¯ã„ã§ã™ã€‚

    ## å†…å®¹ã®æ¡ä»¶
    - ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ${ASSISTANTS.length}äººã®é–“ã ã‘ã§å¯¾è©±ã—ã¦ä¸‹ã•ã„ã€‚ã¤ã¾ã‚Šä»–ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«å‘ã‘ãŸç™ºè¨€ã®ã¿ã‚’ã—ã¦ä¸‹ã•ã„ã€‚
    - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒãã®å ´ã«ã„ãªã„ã‹ã®å¦‚ãè©±ã—ã¦ä¸‹ã•ã„ã€‚
    - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å•é¡Œç‚¹ã‚’æ±ºã‚ã¤ã‘ã‚‹ã“ã¨ã¯ã›ãšã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ‚©ã¿ã®è§£æ±ºã¸ã®ç³¸å£ãŒã©ã“ã«ã‚ã‚‹ã®ã‹æ¨¡ç´¢ã—ã¦ç™ºè¨€ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
    - å¥³æ€§ãŒç™ºè¨€ã™ã‚‹å ´åˆã€ä¸€äººç§°ã¯å¿…ãšã€Œç§ã€ã«ã—ã¦ãã ã•ã„ã€‚
    - å¿ƒä¸­èªã¯å‡ºåŠ›ã›ãšã«ç™ºè¨€å†…å®¹ã ã‘ã‚’å‡ºåŠ›ã™ã‚‹ã‚ˆã†ã«ã—ã¦ä¸‹ã•ã„ã€‚
    - ${WORDINGS_PROMPT}
`;
}

// åå‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰åå‰ã‚’å–å¾—ã™ã‚‹é–¢æ•°
function getNameFromIndex(val: string): string {
    for (const [k, v] of Object.entries(NAME_INDEX)) {
        if (v === val) {
            return k;
        }
    }
    return "unknown";
}

// Azure OpenAI APIã‚’å‘¼ã³å‡ºã™é–¢æ•°
async function chatCompletions(
    client: AzureOpenAI,
    model: string,
    messages: any[],
    altOut: string
): Promise<string> {
    const apiCallStart = performance.now()
    console.log(`ğŸŒ [API] Starting chat completion - Model: ${model}, Messages: ${messages.length}`)
    try {
        const response = await client.chat.completions.create({model, messages});
        const apiCallTime = performance.now() - apiCallStart
        const reply = response.choices[0]?.message?.content;
        console.log(`ğŸŒ [API] Chat completion successful: ${apiCallTime.toFixed(2)}ms`)
        console.log(`ğŸ“ [API] Response length: ${reply?.length || 0} characters`)
        return reply || altOut;
    } catch (error: any) {
        const apiCallTime = performance.now() - apiCallStart
        console.error(`âŒ [API] Azure OpenAI API ã‚¨ãƒ©ãƒ¼ (${apiCallTime.toFixed(2)}ms): ${error}`);
        return altOut;
    }
}

// å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
async function makeResponse(
    client: AzureOpenAI,
    prompt: string,
    conversationLog: any[],
    userInput?: string
): Promise<string> {
    console.log(`âš™ï¸ [MAKE-RESPONSE] Starting message construction`)
    const constructStart = performance.now()
    
    const messages = [
        ...ASSISTANTS.map((assistant, i) => ({
            role: "system",
            name: NAME_INDEX[assistant.name],
            content: `ã‚ãªãŸã¯${assistant.name}ã§ã™ã€‚${CHARACTERS[assistant.name]}`,
        })),
        { role: "system", content: prompt },
        ...conversationLog,
    ];

    if (userInput) {
        messages.push({ role: "user", content: userInput });
    }

    const constructTime = performance.now() - constructStart
    console.log(`âš™ï¸ [MAKE-RESPONSE] Message construction: ${constructTime.toFixed(2)}ms (${messages.length} total messages)`)
    console.log(`ğŸ“ [MAKE-RESPONSE] User input provided: ${!!userInput}`)

    const reply = await chatCompletions(client, AZURE_DEPLOYMENT_NAME, messages, "ï¼š");
    return reply;
}

// è©±è€…ã‚’æ¨æ¸¬ã™ã‚‹é–¢æ•°
async function suggestSpeaker(client: AzureOpenAI, input: string): Promise<string> {
    console.log(`ğŸ­ [SUGGEST-SPEAKER] Inferring speaker for: "${input.substring(0, 50)}..."`)
    const suggestStart = performance.now()
    
    const decisionExamples = `
        å…¥åŠ›ãŒã€Œåƒ•ã§ã™ã‹ï¼Ÿæœ€è¿‘ã¯è¶£å‘³ã®ãƒãƒ©ã‚½ãƒ³ã‚’é ‘å¼µã£ã¦ã‚‹ã‚“ã§ã™ï¼è‡ªç„¶ã®ä¸­ã‚’èµ°ã‚Œã‚‹ã¨ã€é ­ãŒã‚¹ãƒƒã‚­ãƒªã—ã¾ã™ã€‚ã€ã®å ´åˆï¼š
            ä¸€äººç§°ãŒã€Œåƒ•ã€ã§ã‚ã‚Šãƒãƒ©ã‚½ãƒ³ã‚’è¶£å‘³ã«ã—ã¦ã„ã‚‹ã®ã§è©±è€…ã¯${ASSISTANTS[2].name}ã¨åˆ†ã‹ã‚‹ã€‚ã‚ˆã£ã¦å‡ºåŠ›ã¯ã€Œ${ASSISTANTS[2].name}ã€ã€‚
        å…¥åŠ›ãŒã€Œæœ€è¿‘ã¯ã§ã™ã­ï½ã€çŒ«ãŒå¯’ããªã£ã¦ãã¦å¦™ã«ç”˜ãˆã¦ãã‚‹ã‚“ã§ã™ã‚ˆã€‚ä»•äº‹ã‹ã‚‰å¸°ã‚‹ã¨ã‚‚ã†ãšã£ã¨è†ã®ä¸Šã§å‹•ã‹ãªã„ã‚“ã§ã™ã€‚ã»ã£ã“ã‚Šã—ã¾ã™ã€‚ã€ã®å ´åˆï¼š
            çŒ«ã‚’é£¼ã£ã¦ãŠã‚Šå£èª¿ã‚‚ã‚ã¾ã‚Šé‡ã€…ã—ããªã„ã®ã§è©±è€…ã¯${ASSISTANTS[1].name}ã¨åˆ†ã‹ã‚‹ã€‚ã‚ˆã£ã¦å‡ºåŠ›ã¯ã€Œ${ASSISTANTS[1].name}ã€ã€‚
        å…¥åŠ›ãŒã€Œã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€èã„ã¦ãã ã•ã£ã¦ã€‚æœ€è¿‘ã¯å¨˜ãŒå›²ç¢ã‚’å§‹ã‚ã¦ã¾ã—ã¦ã€ä¼‘æ—¥ã«ä¸€ç·’ã«æ‰“ã¤ã“ã¨ãŒå¢—ãˆã¾ã—ãŸã€‚ã¨ã¦ã‚‚æ¥½ã—ã„ã§ã™ã‚ˆã€‚ã€ã®å ´åˆï¼š
            å¨˜ãŒã„ã¦å°†æ£‹ã‚’ã—ã¦ã„ã‚‹ä¸Šã«å£èª¿ã‚‚ä¸å¯§ãªã®ã§è©±è€…ã¯${ASSISTANTS[0].name}ã¨åˆ†ã‹ã‚‹ã€‚ã‚ˆã£ã¦å‡ºåŠ›ã¯ã€Œ${ASSISTANTS[0].name}ã€ã€‚
    `;

    const messages = [
        {
            role: "system",
            content: `å„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®è¨­å®šã¯æ¬¡ã®é€šã‚Šã§ã™ã€‚${JSON.stringify(CHARACTERS)}`,
        },
        {
            role: "system",
            content: `å„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®è¨€è‘‰é£ã„ã¯æ¬¡ã®é€šã‚Šã§ã™ã€‚${WORDINGS_PROMPT}`,
        },
        {
            role: "system",
            content: `å„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å€‹æ€§ã‚„è¨€è‘‰é£ã„ã‚’å¾¹åº•çš„ã«ç†è§£ã—ã¦ä¸‹ã•ã„ã€‚ãã‚Œã‚’ã‚‚ã¨ã«ã€å…¥åŠ›ã•ã‚ŒãŸæ–‡ç« ã‚’ç™ºè¨€ã—ãŸäººç‰©ã¨ã—ã¦æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„äººç‰©ã‚’ç‰¹å®šã—ã€${ASSISTANTS[0].name}ã€${ASSISTANTS[1].name}ã€${ASSISTANTS[2].name}ã®ã„ãšã‚Œã‹ã‚’å‡ºåŠ›ã¨ã—ã¦ä¸‹ã•ã„ã€‚`,
        },
        { role: "system", content: decisionExamples },
        { role: "user", content: input }
    ];

    const speaker = await chatCompletions(client, AZURE_DEPLOYMENT_NAME, messages, "");
    const suggestTime = performance.now() - suggestStart
    console.log(`ğŸ­ [SUGGEST-SPEAKER] Speaker inference completed: ${suggestTime.toFixed(2)}ms - Result: "${speaker}"`)
    return speaker;
}

// ä¼šè©±ãƒ­ã‚°ã‚’æ›´æ–°ã™ã‚‹é–¢æ•°
async function fixComments(
    client: AzureOpenAI,
    userInput: string,
    reply: string,
    conversationLog: any[]
): Promise<string> {
    console.log(`ğŸ”§ [FIX-COMMENTS] Starting comment processing`)
    const fixStart = performance.now()
    
    let results: string = "";
    conversationLog.push({ role: "user", content: userInput });

    const splitStart = performance.now()
    const newLog = reply.split("\n");
    console.log(`ğŸ“„ [FIX-COMMENTS] Split into ${newLog.length} lines: ${(performance.now() - splitStart).toFixed(2)}ms`)
    
    let speakerInferenceCount = 0
    let speakerInferenceTime = 0
    
    for (let i = 0; i < newLog.length; i++) {
        const remark = newLog[i]
        
        // ç©ºè¡Œã¯è€ƒæ…®ã—ãªã„
        if (!remark) {
            continue;
        }

        console.log(`ğŸ” [FIX-COMMENTS] Processing line ${i + 1}: "${remark.substring(0, 30)}..."`)
        const lineStart = performance.now()

        let parts: string[];
        let name: string;
        let content: string;

        // ã€ã€Œå¾Œè—¤ï¼šæœ€è¿‘ã©ã†ã§ã™ã‹ï¼Ÿã€ã€ã‚„ã€ã€Œæœ€è¿‘ã©ã†ã§ã™ã‹ï¼Ÿã€ã€ã®å ´åˆ
        if (remark.startsWith("ã€Œ") && remark.endsWith("ã€")) {
            parts = remark.slice(1, -1).replace(":", "ï¼š").split("ï¼š", 2);
        }
        // ã€å¾Œè—¤ï¼šã€Œæœ€è¿‘ã©ã†ã§ã™ã‹ï¼Ÿã€ã€ã‚„ã€å¾Œè—¤ï¼šæœ€è¿‘ã©ã†ã§ã™ã‹ï¼Ÿã€ã€ã€æœ€è¿‘ã©ã†ã§ã™ã‹ï¼Ÿã€ã®å ´åˆ
        else {
            parts = remark.trim().replace(":", "ï¼š").split("ï¼š", 2);
        }

        if (parts.length === 2 && ASSISTANTS.some(a => a.name === parts[0])) {
            name = parts[0];
            content = parts[1];
            console.log(`âœ… [FIX-COMMENTS] Speaker found directly: ${name}`)
        } else {
            // è©±è€…ãŒç›´æ¥æ›¸ã‹ã‚Œã¦ã„ãªã„ã¨ãã¯remarkã‹ã‚‰suggest_speakerã§æ¨æ¸¬ã™ã‚‹
            console.log(`ğŸ­ [FIX-COMMENTS] Need to infer speaker for: "${remark.substring(0, 30)}..."`)
            const inferStart = performance.now()
            name = await suggestSpeaker(client, remark);
            const inferTime = performance.now() - inferStart
            speakerInferenceTime += inferTime
            speakerInferenceCount++
            
            name = name.trim().replace(/[ã€Œã€ ã€€]/g, "");
            content = parts.length === 2 ? parts[1] : remark;
            console.log(`ğŸ­ [FIX-COMMENTS] Speaker inferred: ${name} (${inferTime.toFixed(2)}ms)`)
        }

        if (content.startsWith("ã€Œ") && content.endsWith("ã€")) {
            content = content.slice(1, -1);
        }

        const nameIndex = NAME_INDEX[name] || "-1";

        const log = { role: "assistant", name: nameIndex, content: content.trim() };
        conversationLog.push(log);

        const result = name + "ï¼š" + content + "\n"
        results += result;
        
        const lineTime = performance.now() - lineStart
        console.log(`â±ï¸ [FIX-COMMENTS] Line ${i + 1} processed: ${lineTime.toFixed(2)}ms`)
    }
    
    const fixTime = performance.now() - fixStart
    console.log(`âœ… [FIX-COMMENTS] Comment processing completed: ${fixTime.toFixed(2)}ms`)
    console.log(`ğŸ“Š [FIX-COMMENTS] Speaker inferences: ${speakerInferenceCount} calls, ${speakerInferenceTime.toFixed(2)}ms total`)
    console.log(`ğŸ“ [FIX-COMMENTS] Final results length: ${results.length} characters`)
    
    return results;
}

// ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼šAzure OpenAIã§conversation_logã¨user_inputã‹ã‚‰å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹
export async function generateOpenAIResponse(
    userInput: string,
    conversationLog: ConversationLog[],
    isReflecting: boolean = false,
    userName: string = USER,
    userGender: string = GENDER
): Promise<string> {
    console.log(`ğŸ”§ [OPENAI] Starting generateOpenAIResponse - Reflecting: ${isReflecting}`)
    const totalStartTime = performance.now()

    const clientInitStart = performance.now()
    const client = new AzureOpenAI({
        apiKey: AZURE_OPENAI_API_KEY,
        baseURL: `${AZURE_OPENAI_ENDPOINT}`,
        defaultQuery: { 'api-version': AZURE_OPENAI_API_VERSION },
        defaultHeaders: {
            'api-key': AZURE_OPENAI_API_KEY,
        },
        dangerouslyAllowBrowser: true
    });
    console.log(`âš™ï¸ [OPENAI] Client initialization: ${(performance.now() - clientInitStart).toFixed(2)}ms`)

    // ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è¨­å®š
    const promptStart = performance.now()
    let count = 1;
    let prompt = createChatPrompt(userName, userGender);
    
    if (isReflecting) {
        count = REFLECTING_CONVERSATION_COUNT;
        prompt = createReflectingPrompt(userName, userGender);
    }
    console.log(`ğŸ“ [OPENAI] Prompt generation: ${(performance.now() - promptStart).toFixed(2)}ms`)

    // ConversationLogã‚’é©åˆ‡ãªå½¢å¼ã«å¤‰æ›
    const formatStart = performance.now()
    const formattedLog = conversationLog.map(log => ({
        role: log.role,
        content: log.content,
        ...(log.speaker && { name: NAME_INDEX[log.speaker.name] })
    }));
    console.log(`ğŸ”„ [OPENAI] Log formatting (${conversationLog.length} messages): ${(performance.now() - formatStart).toFixed(2)}ms`)

    let allResponses = "";
    let totalApiTime = 0
    let totalParsingTime = 0

    console.log(`ğŸ”„ [OPENAI] Starting ${count} iteration(s) of response generation`)
    for (let i = 0; i < count; i++) {
        console.log(`ğŸ”„ [OPENAI] --- Iteration ${i + 1}/${count} ---`)
        const iterationStart = performance.now()
        
        const sentUserInput = i === 0 ? userInput : undefined;
        
        const makeResponseStart = performance.now()
        const comments = await makeResponse(client, prompt, formattedLog, sentUserInput);
        const makeResponseTime = performance.now() - makeResponseStart
        totalApiTime += makeResponseTime
        console.log(`ğŸ¤– [OPENAI] makeResponse iteration ${i + 1}: ${makeResponseTime.toFixed(2)}ms`)
        
        const fixCommentsStart = performance.now()
        const remarks = await fixComments(client, userInput, comments, formattedLog);
        const fixCommentsTime = performance.now() - fixCommentsStart
        totalParsingTime += fixCommentsTime
        console.log(`ğŸ”§ [OPENAI] fixComments iteration ${i + 1}: ${fixCommentsTime.toFixed(2)}ms`)

        // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è“„ç©
        if (allResponses) allResponses += "\n";
        allResponses += remarks;
        
        const iterationTime = performance.now() - iterationStart
        console.log(`â±ï¸ [OPENAI] Iteration ${i + 1} total: ${iterationTime.toFixed(2)}ms`)
    }
    
    if (allResponses.endsWith("\n")) {
        allResponses = allResponses.slice(0, -1);
    }
    
    const totalTime = performance.now() - totalStartTime
    console.log(`âœ… [OPENAI] generateOpenAIResponse completed: ${totalTime.toFixed(2)}ms`)
    console.log(`ğŸ“Š [OPENAI] Breakdown - API calls: ${totalApiTime.toFixed(2)}ms, Parsing: ${totalParsingTime.toFixed(2)}ms`)
    console.log(`ğŸ“„ [OPENAI] Final response length: ${allResponses.length} characters`)
    
    return allResponses;
}

// TTSç”¨ã®Azure OpenAI clientã‚’ä½œæˆã—ã¦éŸ³å£°ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
export async function generateSpeechWithAzureOpenAI(
    text: string,
    speaker: string,
    instructions: string = "æ—¥æœ¬äººã‚‰ã—ã„ç™ºå£°ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"
): Promise<Blob> {
    console.log(`ğŸµ [TTS] Starting speech generation - Speaker: ${speaker}, Text: "${text.substring(0, 30)}..."`)
    const ttsStartTime = performance.now()

    const clientInitStart = performance.now()
    const client = new AzureOpenAI({
        baseURL: AZURE_OPENAI_TTS_ENDPOINT,
        apiKey: AZURE_OPENAI_TTS_API_KEY,
        defaultQuery: { 'api-version': AZURE_OPENAI_TTS_API_VERSION },
        dangerouslyAllowBrowser: true
    });
    console.log(`âš™ï¸ [TTS] TTS Client initialization: ${(performance.now() - clientInitStart).toFixed(2)}ms`)

    const apiCallStart = performance.now()
    const response = await client.audio.speech.create({
        model: AZURE_OPENAI_TTS_DEPLOYMENT_NAME,
        voice: speaker,
        speed: 0.25, // å€¤ã‚’å¤‰ãˆã¦ã‚‚ä½•æ•…ã‹ã‚¹ãƒ”ãƒ¼ãƒ‰å¤‰ã‚ã‚‰ãªã„
        instructions: instructions,
        input: text,
    });
    const apiCallTime = performance.now() - apiCallStart
    console.log(`ğŸŒ [TTS] TTS API call: ${apiCallTime.toFixed(2)}ms`)

    // Blobãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å–å¾—
    const blobStart = performance.now()
    const blobData = await response.blob();
    const blobTime = performance.now() - blobStart
    console.log(`ğŸ“¦ [TTS] Blob conversion: ${blobTime.toFixed(2)}ms`)
    
    const totalTtsTime = performance.now() - ttsStartTime
    console.log(`âœ… [TTS] Speech generation completed: ${totalTtsTime.toFixed(2)}ms`)
    console.log(`ğŸ“Š [TTS] Breakdown - API: ${apiCallTime.toFixed(2)}ms, Blob: ${blobTime.toFixed(2)}ms`)
    console.log(`ğŸ—£ï¸ [TTS] Generated audio size: ${blobData.size} bytes`)
    
    return blobData;
}

// è¤‡æ•°ã®TTSéŸ³å£°ã‚’ä¸¦åˆ—ç”Ÿæˆã™ã‚‹é–¢æ•°
export async function generateMultipleSpeechWithAzureOpenAI(
    speechRequests: Array<{
        text: string;
        speaker: string;
        instructions?: string;
    }>
): Promise<Array<{ blob: Blob; index: number }>> {
    console.log(`ğŸ¯ [PARALLEL-TTS] Starting parallel TTS generation for ${speechRequests.length} requests`);
    const parallelStart = performance.now();

    try {
        // å…¨ã¦ã®éŸ³å£°ç”Ÿæˆã‚’ä¸¦åˆ—å®Ÿè¡Œ
        const speechPromises = speechRequests.map(async (request, index) => {
            console.log(`ğŸ”„ [PARALLEL-TTS] Starting TTS ${index + 1}: ${request.speaker} - "${request.text.substring(0, 30)}..."`);
            const requestStart = performance.now();
            
            try {
                const blob = await generateSpeechWithAzureOpenAI(
                    request.text,
                    request.speaker,
                    request.instructions || "æ—¥æœ¬äººã‚‰ã—ã„ç™ºå£°ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"
                );
                
                const requestTime = performance.now() - requestStart;
                console.log(`âœ… [PARALLEL-TTS] TTS ${index + 1} completed: ${requestTime.toFixed(2)}ms`);
                
                return { blob, index };
            } catch (error) {
                const requestTime = performance.now() - requestStart;
                console.error(`âŒ [PARALLEL-TTS] TTS ${index + 1} failed (${requestTime.toFixed(2)}ms):`, error);
                throw error;
            }
        });

        // å…¨ã¦ã®éŸ³å£°ç”Ÿæˆã®å®Œäº†ã‚’å¾…æ©Ÿ
        const results = await Promise.all(speechPromises);
        
        const parallelTime = performance.now() - parallelStart;
        console.log(`âœ… [PARALLEL-TTS] All parallel TTS generation completed: ${parallelTime.toFixed(2)}ms`);
        console.log(`âš¡ [PARALLEL-TTS] Average time per TTS: ${(parallelTime / speechRequests.length).toFixed(2)}ms`);
        console.log(`ğŸš€ [PARALLEL-TTS] Speedup vs sequential: ~${speechRequests.length}x faster`);
        
        return results;
    } catch (error) {
        const parallelTime = performance.now() - parallelStart;
        console.error(`âŒ [PARALLEL-TTS] Parallel TTS generation failed (${parallelTime.toFixed(2)}ms):`, error);
        throw error;
    }
}
